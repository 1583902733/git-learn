{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCCH-out.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7hxWTvZ5T6NAslZ/Vr/lI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1583902733/python_practice/blob/master/DCCH_out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA4V13R6xhko"
      },
      "source": [
        "import numpy as np\n",
        "import torch.utils.data as util_data\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "\n",
        "def config_dataset(config):\n",
        "    if \"cifar\" in config[\"dataset\"]:\n",
        "        config[\"topK\"] = 5000\n",
        "        config[\"n_class\"] = 10\n",
        "    elif config[\"dataset\"] in [\"nuswide_21\", \"nuswide_21_m\"]:\n",
        "        config[\"topK\"] = 5000\n",
        "        config[\"n_class\"] = 21\n",
        "    elif config[\"dataset\"] == \"nuswide_81_m\":\n",
        "        config[\"topK\"] = 5000\n",
        "        config[\"n_class\"] = 81\n",
        "    elif config[\"dataset\"] == \"coco\":\n",
        "        config[\"topK\"] = 5000\n",
        "        config[\"n_class\"] = 80\n",
        "    elif config[\"dataset\"] == \"imagenet\":\n",
        "        config[\"topK\"] = 1000\n",
        "        config[\"n_class\"] = 100\n",
        "    elif config[\"dataset\"] == \"mirflickr\":\n",
        "        config[\"topK\"] = -1\n",
        "        config[\"n_class\"] = 38\n",
        "    elif config[\"dataset\"] == \"voc2012\":\n",
        "        config[\"topK\"] = -1\n",
        "        config[\"n_class\"] = 20\n",
        "\n",
        "    config[\"data_path\"] = \"/dataset/\" + config[\"dataset\"] + \"/\"\n",
        "    if config[\"dataset\"] == \"nuswide_21\":\n",
        "        config[\"data_path\"] = \"/dataset/NUS-WIDE/\"\n",
        "    if config[\"dataset\"] in [\"nuswide_21_m\", \"nuswide_81_m\"]:\n",
        "        config[\"data_path\"] = \"/dataset/nus_wide_m/\"\n",
        "    if config[\"dataset\"] == \"coco\":\n",
        "        config[\"data_path\"] = \"/dataset/COCO_2014/\"\n",
        "    if config[\"dataset\"] == \"voc2012\":\n",
        "        config[\"data_path\"] = \"/dataset/\"\n",
        "    config[\"data\"] = {\n",
        "        \"train_set\": {\"list_path\": \"./data/\" + config[\"dataset\"] + \"/train.txt\", \"batch_size\": config[\"batch_size\"]},\n",
        "        \"database\": {\"list_path\": \"./data/\" + config[\"dataset\"] + \"/database.txt\", \"batch_size\": config[\"batch_size\"]},\n",
        "        \"test\": {\"list_path\": \"./data/\" + config[\"dataset\"] + \"/test.txt\", \"batch_size\": config[\"batch_size\"]}}\n",
        "    return config\n",
        "\n",
        "\n",
        "class ImageList(object):\n",
        "\n",
        "    def __init__(self, data_path, image_list, transform):\n",
        "        self.imgs = [(data_path + val.split()[0], np.array([int(la) for la in val.split()[1:]])) for val in image_list]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, target = self.imgs[index]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, target, index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "def image_transform(resize_size, crop_size, data_set):\n",
        "    if data_set == \"train_set\":\n",
        "        step = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(crop_size)]\n",
        "    else:\n",
        "        step = [transforms.CenterCrop(crop_size)]\n",
        "    return transforms.Compose([transforms.Resize(resize_size)]\n",
        "                              + step +\n",
        "                              [transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                    std=[0.229, 0.224, 0.225])\n",
        "                               ])\n",
        "\n",
        "\n",
        "class MyCIFAR10(dsets.CIFAR10):\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        img = self.transform(img)\n",
        "        target = np.eye(10, dtype=np.int8)[np.array(target)]\n",
        "        return img, target, index\n",
        "\n",
        "\n",
        "def cifar_dataset(config):\n",
        "    batch_size = config[\"batch_size\"]\n",
        "\n",
        "    train_size = 500\n",
        "    test_size = 100\n",
        "\n",
        "    if config[\"dataset\"] == \"cifar10-2\":\n",
        "        train_size = 5000\n",
        "        test_size = 1000\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(config[\"crop_size\"]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Dataset\n",
        "    train_dataset = MyCIFAR10(root='/dataset/cifar/',\n",
        "                              train=True,\n",
        "                              transform=transform,\n",
        "                              download=True)\n",
        "\n",
        "    test_dataset = MyCIFAR10(root='/dataset/cifar/',\n",
        "                             train=False,\n",
        "                             transform=transform)\n",
        "\n",
        "    database_dataset = MyCIFAR10(root='/dataset/cifar/',\n",
        "                                 train=False,\n",
        "                                 transform=transform)\n",
        "\n",
        "    X = np.concatenate((train_dataset.data, test_dataset.data))\n",
        "    L = np.concatenate((np.array(train_dataset.targets), np.array(test_dataset.targets)))\n",
        "\n",
        "    first = True\n",
        "    for label in range(10):\n",
        "        index = np.where(L == label)[0]\n",
        "\n",
        "        N = index.shape[0]\n",
        "        perm = np.random.permutation(N)\n",
        "        index = index[perm]\n",
        "\n",
        "        if first:\n",
        "            test_index = index[:test_size]\n",
        "            train_index = index[test_size: train_size + test_size]\n",
        "            database_index = index[train_size + test_size:]\n",
        "        else:\n",
        "            test_index = np.concatenate((test_index, index[:test_size]))\n",
        "            train_index = np.concatenate((train_index, index[test_size: train_size + test_size]))\n",
        "            database_index = np.concatenate((database_index, index[train_size + test_size:]))\n",
        "        first = False\n",
        "\n",
        "    if config[\"dataset\"] == \"cifar10\":\n",
        "        # test:1000, train:5000, database:54000\n",
        "        pass\n",
        "    elif config[\"dataset\"] == \"cifar10-1\":\n",
        "        # test:1000, train:5000, database:59000\n",
        "        database_index = np.concatenate((train_index, database_index))\n",
        "    elif config[\"dataset\"] == \"cifar10-2\":\n",
        "        # test:10000, train:50000, database:50000\n",
        "        database_index = train_index\n",
        "\n",
        "    train_dataset.data = X[train_index]\n",
        "    train_dataset.targets = L[train_index]\n",
        "    test_dataset.data = X[test_index]\n",
        "    test_dataset.targets = L[test_index]\n",
        "    database_dataset.data = X[database_index]\n",
        "    database_dataset.targets = L[database_index]\n",
        "\n",
        "    print(\"train_dataset\", train_dataset.data.shape[0])\n",
        "    print(\"test_dataset\", test_dataset.data.shape[0])\n",
        "    print(\"database_dataset\", database_dataset.data.shape[0])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=4)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=4)\n",
        "\n",
        "    database_loader = torch.utils.data.DataLoader(dataset=database_dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=4)\n",
        "\n",
        "    return train_loader, test_loader, database_loader, \\\n",
        "           train_index.shape[0], test_index.shape[0], database_index.shape[0]\n",
        "\n",
        "\n",
        "def get_data(config):\n",
        "    if \"cifar\" in config[\"dataset\"]:\n",
        "        return cifar_dataset(config)\n",
        "\n",
        "    dsets = {}\n",
        "    dset_loaders = {}\n",
        "    data_config = config[\"data\"]\n",
        "\n",
        "    for data_set in [\"train_set\", \"test\", \"database\"]:\n",
        "        dsets[data_set] = ImageList(config[\"data_path\"],\n",
        "                                    open(data_config[data_set][\"list_path\"]).readlines(),\n",
        "                                    transform=image_transform(config[\"resize_size\"], config[\"crop_size\"], data_set))\n",
        "        print(data_set, len(dsets[data_set]))\n",
        "        dset_loaders[data_set] = util_data.DataLoader(dsets[data_set],\n",
        "                                                      batch_size=data_config[data_set][\"batch_size\"],\n",
        "                                                      shuffle=True, num_workers=4)\n",
        "\n",
        "    return dset_loaders[\"train_set\"], dset_loaders[\"test\"], dset_loaders[\"database\"], \\\n",
        "           len(dsets[\"train_set\"]), len(dsets[\"test\"]), len(dsets[\"database\"])\n",
        "\n",
        "\n",
        "def compute_result(dataloader, net, device):\n",
        "    bs, clses = [], []\n",
        "    net.eval()\n",
        "    for img, cls, _ in tqdm(dataloader):\n",
        "        clses.append(cls)\n",
        "        bs.append((net(img.to(device))).data.cpu())\n",
        "    return torch.cat(bs).sign(), torch.cat(clses)\n",
        "\n",
        "#计算海明距离\n",
        "def CalcHammingDist(B1, B2):\n",
        "    q = B2.shape[1]\n",
        "    distH = 0.5 * (q - np.dot(B1, B2.transpose()))\n",
        "    return distH\n",
        "\n",
        "\n",
        "def CalcTopMap(rB, qB, retrievalL, queryL, topk):\n",
        "    num_query = queryL.shape[0]\n",
        "    topkmap = 0\n",
        "    for iter in tqdm(range(num_query)):\n",
        "        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n",
        "        hamm = CalcHammingDist(qB[iter, :], rB)\n",
        "        ind = np.argsort(hamm)\n",
        "        gnd = gnd[ind]\n",
        "\n",
        "        tgnd = gnd[0:topk]\n",
        "        tsum = np.sum(tgnd).astype(int)\n",
        "        if tsum == 0:\n",
        "            continue\n",
        "        count = np.linspace(1, tsum, tsum)\n",
        "\n",
        "        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n",
        "        topkmap_ = np.mean(count / (tindex))\n",
        "        topkmap = topkmap + topkmap_\n",
        "    topkmap = topkmap / num_query\n",
        "    return topkmap\n",
        "\n",
        "\n",
        "\n",
        "def mean_average_precision(query_code,\n",
        "                           database_code,\n",
        "                           query_labels,\n",
        "                           database_labels,\n",
        "                           device,\n",
        "                           topk=None,\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Calculate mean average precision(map).\n",
        "    Args:\n",
        "        query_code (torch.Tensor): Query data hash code.\n",
        "        database_code (torch.Tensor): Database data hash code.\n",
        "        query_labels (torch.Tensor): Query data targets, one-hot\n",
        "        database_labels (torch.Tensor): Database data targets, one-host\n",
        "        device (torch.device): Using CPU or GPU.\n",
        "        topk (int): Calculate top k data map.\n",
        "    Returns:\n",
        "        meanAP (float): Mean Average Precision.\n",
        "    \"\"\"\n",
        "    num_query = query_labels.shape[0]\n",
        "    mean_AP = 0.0\n",
        "\n",
        "    for i in range(num_query):\n",
        "        # Retrieve images from database\n",
        "        retrieval = (query_labels[i, :] @ database_labels.t() > 0).float()\n",
        "\n",
        "        # Calculate hamming distance\n",
        "        hamming_dist = 0.5 * (database_code.shape[1] - query_code[i, :] @ database_code.t())\n",
        "\n",
        "        # Arrange position according to hamming distance\n",
        "        retrieval = retrieval[torch.argsort(hamming_dist)][:topk]\n",
        "\n",
        "        # Retrieval count\n",
        "        retrieval_cnt = retrieval.sum().int().item()\n",
        "\n",
        "        # Can not retrieve images\n",
        "        if retrieval_cnt == 0:\n",
        "            continue\n",
        "\n",
        "        # Generate score for every position\n",
        "        score = torch.linspace(1, retrieval_cnt, retrieval_cnt).to(device)\n",
        "\n",
        "        # Acquire index\n",
        "        index = (torch.nonzero(retrieval == 1).squeeze() + 1.0).float().to(device)\n",
        "\n",
        "        mean_AP += (score / index).mean()\n",
        "\n",
        "    mean_AP = mean_AP / num_query\n",
        "    return mean_AP"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I41XDOTxlGs"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, hash_bit, pretrained=True):\n",
        "        super(AlexNet, self).__init__()\n",
        "\n",
        "        model_alexnet = models.alexnet(pretrained=pretrained)\n",
        "        self.features = model_alexnet.features\n",
        "        cl1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        cl1.weight = model_alexnet.classifier[1].weight\n",
        "        cl1.bias = model_alexnet.classifier[1].bias\n",
        "\n",
        "        cl2 = nn.Linear(4096, 4096)\n",
        "        cl2.weight = model_alexnet.classifier[4].weight\n",
        "        cl2.bias = model_alexnet.classifier[4].bias\n",
        "\n",
        "        self.hash_layer = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            cl1,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            cl2,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, hash_bit),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.hash_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "resnet_dict = {\"ResNet18\": models.resnet18, \"ResNet34\": models.resnet34, \"ResNet50\": models.resnet50,\n",
        "               \"ResNet101\": models.resnet101, \"ResNet152\": models.resnet152}\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, hash_bit, res_model=\"ResNet50\"):\n",
        "        super(ResNet, self).__init__()\n",
        "        model_resnet = resnet_dict[res_model](pretrained=True)\n",
        "        self.conv1 = model_resnet.conv1\n",
        "        self.bn1 = model_resnet.bn1\n",
        "        self.relu = model_resnet.relu\n",
        "        self.maxpool = model_resnet.maxpool\n",
        "        self.layer1 = model_resnet.layer1\n",
        "        self.layer2 = model_resnet.layer2\n",
        "        self.layer3 = model_resnet.layer3\n",
        "        self.layer4 = model_resnet.layer4\n",
        "        self.avgpool = model_resnet.avgpool\n",
        "        self.feature_layers = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, \\\n",
        "                                            self.layer1, self.layer2, self.layer3, self.layer4, self.avgpool)\n",
        "\n",
        "        self.hash_layer = nn.Linear(model_resnet.fc.in_features, hash_bit)\n",
        "        self.hash_layer.weight.data.normal_(0, 0.01)\n",
        "        self.hash_layer.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y = self.hash_layer(x)\n",
        "        return y\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56qybRovxoXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cbb3e7-1e76-4a0d-f08c-e86324e5dcac"
      },
      "source": [
        "# from utils.tools import *\n",
        "# from network import *\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "\n",
        "# DCH(CVPR2018)\n",
        "# paper [Deep Cauchy Hashing for Hamming Space Retrieval](http://ise.thss.tsinghua.edu.cn/~mlong/doc/deep-cauchy-hashing-cvpr18.pdf)\n",
        "# official code [DCH-tensorflow](https://github.com/thulab/DeepHash)\n",
        "# code [DCH--pytorch](https://github.com/3140102441/DCH--pytorch)\n",
        "\n",
        "# [DCH] epoch:150, bit:48, dataset:cifar10-1, MAP:0.768, Best MAP: 0.810\n",
        "# [DCH] epoch:150, bit:48, dataset:coco, MAP:0.665, Best MAP: 0.670\n",
        "# [DCH] epoch:150, bit:48, dataset:imagenet, MAP:0.586, Best MAP: 0.586\n",
        "# [DCH] epoch:150, bit:48, dataset:nuswide_21, MAP:0.778, Best MAP: 0.794\n",
        "\n",
        "def get_config():\n",
        "    config = {\n",
        "        \"gamma\": 20.0,\n",
        "        \"lambda\": 0.1,\n",
        "        \"beta\": 1,\n",
        "        # \"optimizer\": {\"type\": optim.SGD, \"optim_params\": {\"lr\": 0.005, \"weight_decay\": 1e-5}},\n",
        "        \"optimizer\": {\"type\": optim.RMSprop, \"optim_params\": {\"lr\": 1e-5, \"weight_decay\": 1e-5}},\n",
        "        \"info\": \"[DCCH]\",\n",
        "        \"resize_size\": 256,\n",
        "        \"crop_size\": 224,\n",
        "        \"batch_size\": 128,\n",
        "        \"net\": AlexNet,\n",
        "        # \"net\":ResNet,\n",
        "        \"save_path\": \"./model\"\n",
        "        \"dataset\": \"cifar10-1\",\n",
        "        # \"dataset\": \"cifar10\",\n",
        "        # \"dataset\": \"coco\",\n",
        "        # \"dataset\": \"imagenet\",\n",
        "        # \"dataset\": \"nuswide_21\",\n",
        "        \"epoch\": 150,\n",
        "        \"test_map\": 10,\n",
        "        # \"device\":torch.device(\"cpu\"),\n",
        "        \"device\": torch.device(\"cuda\"),\n",
        "        \"bit_list\": [48],\n",
        "    }\n",
        "    config = config_dataset(config)\n",
        "    return config\n",
        "\n",
        "\n",
        "class DCHLoss(torch.nn.Module):\n",
        "    def __init__(self, config, bit):\n",
        "        super(DCHLoss, self).__init__()\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.lambda1 = config[\"lambda\"]\n",
        "        self.beta = config[\"beta\"]\n",
        "        self.K = bit\n",
        "        self.one = torch.ones((config[\"batch_size\"], bit)).to(config[\"device\"])\n",
        "        self.fc = torch.nn.Linear(bit, config[\"n_class\"], bias=False).to(config[\"device\"])\n",
        "\n",
        "    def d(self, hi, hj):\n",
        "        inner_product = hi @ hj.t()\n",
        "        norm = hi.pow(2).sum(dim=1, keepdim=True).pow(0.5) @ hj.pow(2).sum(dim=1, keepdim=True).pow(0.5).t()\n",
        "        cos = inner_product / norm.clamp(min=0.0001)\n",
        "        # formula 6\n",
        "        return (1 - cos.clamp(max=0.99)) * self.K / 2\n",
        "\n",
        "    def forward(self, u, y, ind, config):\n",
        "        s = (y @ y.t() > 0).float()\n",
        "\n",
        "        if (1 - s).sum() != 0 and s.sum() != 0:\n",
        "            # formula 2\n",
        "            positive_w = s * s.numel() / s.sum()\n",
        "            negative_w = (1 - s) * s.numel() / (1 - s).sum()\n",
        "            w = positive_w + negative_w\n",
        "        else:\n",
        "            # maybe |S1|==0 or |S2|==0\n",
        "            w = 1\n",
        "\n",
        "        d_hi_hj = self.d(u, u)\n",
        "        # formula 8\n",
        "        cauchy_loss = w * (s * torch.log(d_hi_hj / self.gamma) + torch.log(1 + self.gamma / d_hi_hj))\n",
        "        # formula 9\n",
        "        quantization_loss = torch.log(1 + self.d(u.abs(), self.one) / self.gamma)\n",
        "\n",
        "        o = self.fc(u)\n",
        "        if \"nuswide_21\" in config[\"dataset\"]:\n",
        "            # formula 8, multiple labels classification loss\n",
        "            Lc = (o - y * o + ((1 + (-o).exp()).log())).sum(dim=1).mean()\n",
        "        else:\n",
        "            # formula 7, single labels classification loss\n",
        "            # Lc = (-o.softmax(dim=1).log() * y).sum(dim=1).mean()\n",
        "            CSE = nn.CrossEntropyLoss()\n",
        "            label = torch.topk(y.long(), 1)[1].squeeze(1)\n",
        "            Lc = CSE(o,label)\n",
        "\n",
        "        # formula 7\n",
        "        loss = self.lambda1 * cauchy_loss.mean() + self.lambda1 * quantization_loss.mean() + self.beta * Lc\n",
        "        \n",
        "        return loss\n",
        "\n",
        "\n",
        "def train_val(config, bit):\n",
        "    device = config[\"device\"]\n",
        "    train_loader, test_loader, dataset_loader, num_train, num_test, num_dataset = get_data(config)\n",
        "    config[\"num_train\"] = num_train\n",
        "    net = config[\"net\"](bit).to(device)\n",
        "\n",
        "    optimizer = config[\"optimizer\"][\"type\"](net.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
        "\n",
        "    criterion = DCHLoss(config, bit)\n",
        "\n",
        "    Best_mAP = 0\n",
        "    myloss = []\n",
        "\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "\n",
        "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
        "\n",
        "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
        "            config[\"info\"], epoch + 1, config[\"epoch\"], current_time, bit, config[\"dataset\"]), end=\"\")\n",
        "\n",
        "        net.train()\n",
        "\n",
        "        train_loss = 0\n",
        "        for image, label, ind in train_loader:\n",
        "            image = image.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            u = net(image)\n",
        "\n",
        "            loss = criterion(u, label.float(), ind, config)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        myloss.append(train_loss)\n",
        "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.3f\" % (train_loss))\n",
        "\n",
        "        if epoch>90 and (epoch + 1) % config[\"test_map\"] == 0:\n",
        "            # print(\"calculating test binary code......\")\n",
        "            tst_binary, tst_label = compute_result(test_loader, net, device=device)\n",
        "\n",
        "            # print(\"calculating dataset binary code.......\")\\\n",
        "            trn_binary, trn_label = compute_result(dataset_loader, net, device=device)\n",
        "\n",
        "            # print(\"calculating map.......\")\n",
        "            # mAP = CalcTopMap(trn_binary.numpy(), tst_binary.numpy(), trn_label.numpy(), tst_label.numpy(),\n",
        "            #                  config[\"topK\"])\n",
        "            mAP = mean_average_precision(tst_binary,trn_binary,tst_label,trn_label,device,config[\"topK\"])\n",
        "            if mAP > Best_mAP:\n",
        "                Best_mAP = mAP\n",
        "\n",
        "                if \"save_path\" in config:\n",
        "                    if not os.path.exists(config[\"save_path\"]):\n",
        "                        os.makedirs(config[\"save_path\"])\n",
        "                    print(\"save in \", config[\"save_path\"])\n",
        "                    np.save(os.path.join(config[\"save_path\"], config[\"dataset\"] + str(mAP) + \"-\" + \"trn_binary.npy\"),\n",
        "                            trn_binary.numpy())\n",
        "                    torch.save(net.state_dict(),\n",
        "                               os.path.join(config[\"save_path\"], config[\"dataset\"] + \"-\" + str(mAP) + \"-model.pt\"))\n",
        "            print(\"%s epoch:%d, bit:%d, dataset:%s, MAP:%.3f, Best MAP: %.3f\" % (\n",
        "                config[\"info\"], epoch + 1, bit, config[\"dataset\"], mAP, Best_mAP))\n",
        "            print(config)\n",
        "    myloss = np.array(myloss)\n",
        "    np.save(\"./loss{}\".format(bit),myloss)\n",
        "\n",
        "\n",
        "config = get_config()\n",
        "print(config)\n",
        "for bit in config[\"bit_list\"]:\n",
        "    train_val(config, bit)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}}\n",
            "Files already downloaded and verified\n",
            "train_dataset 5000\n",
            "test_dataset 1000\n",
            "database_dataset 59000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH][ 1/100][03:57:09] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:1.424\n",
            "[DCH][ 2/100][03:57:17] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.850\n",
            "[DCH][ 3/100][03:57:25] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.736\n",
            "[DCH][ 4/100][03:57:34] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.656\n",
            "[DCH][ 5/100][03:57:42] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.614\n",
            "[DCH][ 6/100][03:57:51] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.576\n",
            "[DCH][ 7/100][03:57:59] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.563\n",
            "[DCH][ 8/100][03:58:08] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.511\n",
            "[DCH][ 9/100][03:58:16] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.475\n",
            "[DCH][10/100][03:58:24] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.14it/s]\n",
            "100%|██████████| 461/461 [01:29<00:00,  5.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:10, bit:48, dataset:cifar10-1, MAP:0.606, Best MAP: 0.606\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][11/100][04:00:09] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.418\n",
            "[DCH][12/100][04:00:18] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.406\n",
            "[DCH][13/100][04:00:26] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.388\n",
            "[DCH][14/100][04:00:35] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.363\n",
            "[DCH][15/100][04:00:43] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.339\n",
            "[DCH][16/100][04:00:52] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.314\n",
            "[DCH][17/100][04:01:01] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.300\n",
            "[DCH][18/100][04:01:09] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.299\n",
            "[DCH][19/100][04:01:18] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.280\n",
            "[DCH][20/100][04:01:27] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.24it/s]\n",
            "100%|██████████| 461/461 [01:33<00:00,  4.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:20, bit:48, dataset:cifar10-1, MAP:0.633, Best MAP: 0.633\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][21/100][04:03:16] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.257\n",
            "[DCH][22/100][04:03:25] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.236\n",
            "[DCH][23/100][04:03:34] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.231\n",
            "[DCH][24/100][04:03:42] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.218\n",
            "[DCH][25/100][04:03:51] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.205\n",
            "[DCH][26/100][04:04:00] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.209\n",
            "[DCH][27/100][04:04:09] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.202\n",
            "[DCH][28/100][04:04:18] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.183\n",
            "[DCH][29/100][04:04:26] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.184\n",
            "[DCH][30/100][04:04:35] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.16it/s]\n",
            "100%|██████████| 461/461 [01:32<00:00,  4.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:30, bit:48, dataset:cifar10-1, MAP:0.659, Best MAP: 0.659\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][31/100][04:06:23] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.177\n",
            "[DCH][32/100][04:06:32] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.174\n",
            "[DCH][33/100][04:06:41] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.163\n",
            "[DCH][34/100][04:06:49] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.162\n",
            "[DCH][35/100][04:06:58] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.156\n",
            "[DCH][36/100][04:07:07] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.153\n",
            "[DCH][37/100][04:07:16] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.151\n",
            "[DCH][38/100][04:07:25] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.145\n",
            "[DCH][39/100][04:07:34] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.143\n",
            "[DCH][40/100][04:07:43] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:02<00:00,  3.68it/s]\n",
            "100%|██████████| 461/461 [01:33<00:00,  4.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:40, bit:48, dataset:cifar10-1, MAP:0.682, Best MAP: 0.682\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][41/100][04:09:32] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.137\n",
            "[DCH][42/100][04:09:41] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.135\n",
            "[DCH][43/100][04:09:50] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.139\n",
            "[DCH][44/100][04:09:59] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.138\n",
            "[DCH][45/100][04:10:07] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.132\n",
            "[DCH][46/100][04:10:16] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.130\n",
            "[DCH][47/100][04:10:24] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.128\n",
            "[DCH][48/100][04:10:33] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.127\n",
            "[DCH][49/100][04:10:42] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.126\n",
            "[DCH][50/100][04:10:50] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.12it/s]\n",
            "100%|██████████| 461/461 [01:32<00:00,  4.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:50, bit:48, dataset:cifar10-1, MAP:0.707, Best MAP: 0.707\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][51/100][04:12:38] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.125\n",
            "[DCH][52/100][04:12:46] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.123\n",
            "[DCH][53/100][04:12:55] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.118\n",
            "[DCH][54/100][04:13:03] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.118\n",
            "[DCH][55/100][04:13:12] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.117\n",
            "[DCH][56/100][04:13:20] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.117\n",
            "[DCH][57/100][04:13:29] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.118\n",
            "[DCH][58/100][04:13:37] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.116\n",
            "[DCH][59/100][04:13:46] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.117\n",
            "[DCH][60/100][04:13:54] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.18it/s]\n",
            "100%|██████████| 461/461 [01:33<00:00,  4.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:60, bit:48, dataset:cifar10-1, MAP:0.731, Best MAP: 0.731\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][61/100][04:15:43] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.112\n",
            "[DCH][62/100][04:15:52] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.112\n",
            "[DCH][63/100][04:16:00] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.110\n",
            "[DCH][64/100][04:16:09] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.110\n",
            "[DCH][65/100][04:16:18] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.110\n",
            "[DCH][66/100][04:16:27] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.111\n",
            "[DCH][67/100][04:16:35] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.112\n",
            "[DCH][68/100][04:16:44] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.111\n",
            "[DCH][69/100][04:16:53] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.107\n",
            "[DCH][70/100][04:17:01] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.09it/s]\n",
            "100%|██████████| 461/461 [01:32<00:00,  5.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:70, bit:48, dataset:cifar10-1, MAP:0.750, Best MAP: 0.750\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][71/100][04:18:48] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.104\n",
            "[DCH][72/100][04:18:57] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.105\n",
            "[DCH][73/100][04:19:06] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.102\n",
            "[DCH][74/100][04:19:14] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.105\n",
            "[DCH][75/100][04:19:23] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.106\n",
            "[DCH][76/100][04:19:31] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.104\n",
            "[DCH][77/100][04:19:40] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.101\n",
            "[DCH][78/100][04:19:48] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.103\n",
            "[DCH][79/100][04:19:57] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.099\n",
            "[DCH][80/100][04:20:06] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.15it/s]\n",
            "100%|██████████| 461/461 [01:32<00:00,  5.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:80, bit:48, dataset:cifar10-1, MAP:0.765, Best MAP: 0.765\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][81/100][04:21:53] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.099\n",
            "[DCH][82/100][04:22:01] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.099\n",
            "[DCH][83/100][04:22:10] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.098\n",
            "[DCH][84/100][04:22:19] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.096\n",
            "[DCH][85/100][04:22:27] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.100\n",
            "[DCH][86/100][04:22:36] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.098\n",
            "[DCH][87/100][04:22:45] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.103\n",
            "[DCH][88/100][04:22:53] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.095\n",
            "[DCH][89/100][04:23:02] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.094\n",
            "[DCH][90/100][04:23:10] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.06it/s]\n",
            "100%|██████████| 461/461 [01:31<00:00,  5.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:90, bit:48, dataset:cifar10-1, MAP:0.799, Best MAP: 0.799\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n",
            "[DCH][91/100][04:24:57] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.094\n",
            "[DCH][92/100][04:25:05] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.092\n",
            "[DCH][93/100][04:25:14] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.091\n",
            "[DCH][94/100][04:25:22] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.092\n",
            "[DCH][95/100][04:25:31] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.091\n",
            "[DCH][96/100][04:25:40] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.091\n",
            "[DCH][97/100][04:25:48] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.091\n",
            "[DCH][98/100][04:25:57] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.089\n",
            "[DCH][99/100][04:26:05] bit:48, dataset:cifar10-1, training....\b\b\b\b\b\b\b loss:0.088\n",
            "[DCH][100/100][04:26:14] bit:48, dataset:cifar10-1, training...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b loss:0.088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  4.20it/s]\n",
            "100%|██████████| 461/461 [01:30<00:00,  5.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[DCH] epoch:100, bit:48, dataset:cifar10-1, MAP:0.812, Best MAP: 0.812\n",
            "{'gamma': 20.0, 'lambda': 0.1, 'beta': 1, 'optimizer': {'type': <class 'torch.optim.rmsprop.RMSprop'>, 'optim_params': {'lr': 1e-05, 'weight_decay': 1e-05}}, 'info': '[DCH]', 'resize_size': 256, 'crop_size': 224, 'batch_size': 128, 'net': <class '__main__.AlexNet'>, 'dataset': 'cifar10-1', 'epoch': 100, 'test_map': 10, 'device': device(type='cuda'), 'bit_list': [48], 'topK': 5000, 'n_class': 10, 'data_path': '/dataset/cifar10-1/', 'data': {'train_set': {'list_path': './data/cifar10-1/train.txt', 'batch_size': 128}, 'database': {'list_path': './data/cifar10-1/database.txt', 'batch_size': 128}, 'test': {'list_path': './data/cifar10-1/test.txt', 'batch_size': 128}}, 'num_train': 5000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msCqIrKsx2Oh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "9fff4c0c-3b2e-4cb7-d00e-8426aec15a98"
      },
      "source": [
        "net"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d3e61c39d23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4aXtxJl1OP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}